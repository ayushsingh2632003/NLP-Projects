{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushsingh2632003/NLP-Projects/blob/main/Vocabulary_and_Matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLLKQ3azwPPV"
      },
      "source": [
        "# 1) Rule-Based Matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMgckZZXwmjf"
      },
      "source": [
        "spaCy’s rule-based matcher engines and components not only let you find the words and phrases you’re looking for\n",
        ",they also give you access to the tokens within the document and their relationships\n",
        "\n",
        "This means you can easily access and analyze the surrounding tokens, merge spans into single tokens or add entries to the named entities in doc.ents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE73bDPDV8yv"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBL1fOTbO9on"
      },
      "source": [
        "# Import the Matcher library\n",
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab) # created matcher object and pass nlp.vocab\n",
        "\n",
        "# Here matcher is an object that pairs to current Vocab object\n",
        "# We can add and remove specific named matchers to matcher as needed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3gQj0DIbwGN"
      },
      "source": [
        "## Creating patterns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVp7sLzTbtaj"
      },
      "source": [
        "# create a list, and inside that list add series of dictionaries\n",
        "\n",
        "# Hello World can appear in the following ways,\n",
        "# 1) Hello World\n",
        "# 2) Hello-World\n",
        "\n",
        "pattern_1 = [{'LOWER': 'hello'}, {'LOWER': 'world'}]\n",
        "pattern_2 = [{'LOWER': 'hello'}, {'IS_PUNCT': True}, {'LOWER': 'world'}]\n",
        "\n",
        "# 'LOWER', 'IS_PUNCT' are the attributes\n",
        "# they has to be written in  that way only"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxApvK08fItL"
      },
      "source": [
        "# Add patterns to matcher object\n",
        "\n",
        "# Add a match rule to matcher, A match rule consists of,\n",
        "# 1) An ID key\n",
        "# 2) an on_match callback\n",
        "# 3) one or more patterns\n",
        "\n",
        "matcher.add('Hello World', None, pattern_1, pattern_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJg4Jd8Vgjaw"
      },
      "source": [
        "# create a document\n",
        "doc = nlp(\" 'Hello World' are the first two printed words for most of the programmers, printing 'Hello-World' is most common for beginners\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRGW4ZNdeDDd",
        "outputId": "eafa6506-4d67-4e45-e0d7-1ac39e39ab4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "doc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 'Hello World' are the first two printed words for most of the programmers, printing 'Hello-World' is most common for beginners"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz_S5KMVnndZ"
      },
      "source": [
        "## finding the matches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXxCyjlqrtzP",
        "outputId": "ba23b23a-fb5a-4fb8-d7e7-7f046c62dded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "find_matches = matcher(doc) # passin doc to matcher object and store this in a variable\n",
        "print(find_matches)\n",
        "\n",
        "# it returns output list of tuples\n",
        "# string ID, index start and index end"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(8585552006568828647, 2, 4), (8585552006568828647, 19, 22)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4Xz0U2hl1Nv",
        "outputId": "0a55a100-8163-4696-f224-7b3ae6896b55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# define a function to find the matches\n",
        "\n",
        "for match_id, start, end in find_matches:\n",
        "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "    span = doc[start:end]                    # get the matched span\n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8585552006568828647 Hello World 2 4 Hello World\n",
            "8585552006568828647 Hello World 19 22 Hello-World\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GF8u-F3nvRC"
      },
      "source": [
        "# Removing the matches\n",
        "matcher.remove('Hello World')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc8jL0FvoH9J"
      },
      "source": [
        "## Setting pattern options and quantifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWRmOxHboJi5"
      },
      "source": [
        "# Redefine the patterns:\n",
        "pattern_3 = [{'LOWER': 'hello'}, {'LOWER': 'world'}]\n",
        "pattern_4 = [{'LOWER': 'hello'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'world'}]\n",
        "# 'OP':'*' ----> Thisis going to allow this pattern to match zero or more times for any punctuation\n",
        "\n",
        "# Add the new set of patterns to the 'Hellow World' matcher:\n",
        "matcher.add('Hello World', None, pattern_3, pattern_4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3B1QXLVoMVc"
      },
      "source": [
        "doc_2 = nlp(\"You can print Hello World or hello world or Hello-World\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oac-XxG8qTuF",
        "outputId": "62c38bca-162f-48ba-cebc-9211bebb67ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "find_matches = matcher(doc_2)\n",
        "print(find_matches)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(8585552006568828647, 3, 5), (8585552006568828647, 6, 8), (8585552006568828647, 9, 12)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrZp0yjVv22g"
      },
      "source": [
        "# 2) Phrase Matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoJS53ipFlxB"
      },
      "source": [
        "In the above section we used token patterns to perform rule-based matching. An alternative and more efficient method is to match on terminology lists\n",
        "\n",
        "In this case we use PhraseMatcher to create a Doc object from a list of phrases, and pass that into matcher instead\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjTSL_-0FxU5"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM9hCo7PF4zY"
      },
      "source": [
        "# Import the PhraseMatcher library\n",
        "from spacy.matcher import PhraseMatcher\n",
        "matcher = PhraseMatcher(nlp.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGxbT7_JF6_m"
      },
      "source": [
        "phrase_list = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRfdLc-PHHjH"
      },
      "source": [
        "# Convert each phrase to a document object\n",
        "phrase_patterns = [nlp(text) for text in phrase_list] # to do that we are using list comprehension"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hTe16VdInID",
        "outputId": "54cc2ac5-68b3-4dd3-e376-92ee8c893474",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "phrase_patterns\n",
        "# phrase objects are not strings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Barack Obama, Angela Merkel, Washington, D.C.]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q3ozY6WJF6B",
        "outputId": "e5edca26-e0f2-4abf-ce24-cc47dc9850b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(phrase_patterns[0])\n",
        "# they are the spacy docs\n",
        "# thats why we don't have any quotes there"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIFbvD9yIdcV"
      },
      "source": [
        "# pass each doc object into the matcher\n",
        "matcher.add(\"TerminologyList\", None, *phrase_patterns)\n",
        "# thats we have to add asterisk mark before phrase_pattern"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY4A1TI-HJKQ"
      },
      "source": [
        "doc_3 = nlp(\"German Chancellor Angela Merkel and US President Barack Obama \"\n",
        "          \"converse in the Oval Office inside the White House in Washington, D.C.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsoblgiAHMXg",
        "outputId": "a49c4411-cf2a-4b98-f8ef-2c218c29cf32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "find_matches = matcher(doc_3) # passin doc to matcher object and store this in a variable\n",
        "print(find_matches)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(3766102292120407359, 2, 4), (3766102292120407359, 7, 9), (3766102292120407359, 19, 22)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCpTU7i-KaLm",
        "outputId": "f2a300cd-badc-4ec0-ce1c-d462f6a496f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# define a function to find the matches\n",
        "\n",
        "for match_id, start, end in find_matches:\n",
        "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "    span = doc_3[start:end]                    # get the matched span\n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3766102292120407359 TerminologyList 2 4 Angela Merkel\n",
            "3766102292120407359 TerminologyList 7 9 Barack Obama\n",
            "3766102292120407359 TerminologyList 19 22 Washington, D.C.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}